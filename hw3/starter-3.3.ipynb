{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not change imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing, decomposition\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Import Data\n",
    "\n",
    "For this assignment we will use the Iris [dataset](http://archive.ics.uci.edu/ml/datasets/Iris)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data - directly from the web!\n",
    "#                 column names are separate, so we add them manually\n",
    "data = pd.read_csv(\n",
    "    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
    "    header=None,\n",
    "    names = ['sepal_l_cm', 'sepal_w_cm', 'petal_l_cm', 'petal_w_cm', 'species'],\n",
    "    sep=',')\n",
    "\n",
    "# Output the data (notice Pandas will split long files)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So as to not fool ourselves, let's split on input data/class label\n",
    "# In an unsupervised setting, we might not have the class label,\n",
    "# or use it only for final evaluation\n",
    "X = data.iloc[:,0:-1]\n",
    "Y = data.iloc[:, -1]\n",
    "data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just to confirm...\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Data Exploration via Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Use the describe method to output a table of basic stats on all of X's columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Use the hist method to output histograms of all of X's columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.1 Supervised Visualization\n",
    "While not something we can always do, let's plot each feature with its corresponding species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distinct_species = Y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for species in distinct_species:\n",
    "    plt.hist([X.loc[p].sepal_l_cm for p in Y[Y.isin([species])].index], label=species)\n",
    "\n",
    "plt.title('Sepal Length')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: repeat for sepal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: repeat for petal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: repeat for petal width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: given these visualizations, what conclusion can you draw with respect to the potential for dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Preprocessing with `scikit-learn`\n",
    "Before we apply PCA, all features should be normalized such that mean=0, std=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Use the fit_transform function of a preprocessing.StandardScaler to normalize X\n",
    "#       Replace assignment\n",
    "X_std = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the newly standardized data back into a Pandas DataFrame data structure for convenience\n",
    "X_std = pd.DataFrame(X_std, columns=list(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: use describe to confirm mean/variance has changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: use hist to confirm that the columns have the same relative shapes as before preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Computing the Covariance Matrix\n",
    "In order to perform PCA with the eigendecomposition, we will need the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: use NumPy's cov function to compute the covariance matrix of X_std\n",
    "#       NOTE! it expects each row to be all the values of a feature\n",
    "#             SO... give it the transpose of X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But of course we will do this by hand :)\n",
    "\n",
    "$ \\Sigma = \\frac{1}{n-1} [ (X - \\bar{X})^\\intercal(X - \\bar{X}) ] $\n",
    "\n",
    "But remember, we've already subtracted the mean (for `X_std`), so...\n",
    "\n",
    "$ \\Sigma = \\frac{1}{n-1} ( X^\\intercal X ) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: get n via the shape attribute\n",
    "n = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: a quick way to perform matrix multiplication is the @ operator\n",
    "#       NOTE! access the data in the DataFrame via the \"values\" attribute\n",
    "X_std_cov = None\n",
    "\n",
    "# check that your covariance = NumPy above\n",
    "X_std_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: why is this less than an ideal matrix to compute for real datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. Eigendecomposition vs SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: use the eig linear algebra function of NumPy to compute the eigendecomposition\n",
    "eigvals, eigvecs = (None, None)\n",
    "\n",
    "print(\"Values:\", eigvals)\n",
    "print()\n",
    "print(\"Vectors:\", eigvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: use the svd linear algebra function of NumPy to compute the SVD\n",
    "#       Note! as with cov, use the transpose\n",
    "svd_u,svd_s,svd_v = (None, None, None)\n",
    "\n",
    "print(\"U:\", svd_u)\n",
    "print()\n",
    "print(\"S:\", svd_s)\n",
    "print()\n",
    "print(\"V:\", svd_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "1. Comment on what components of the eigendecomposition correspond to what components of the SVD\n",
    "2. For each of these, note that the values are not identical - why is this ok with respect to how we use/interpret the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F. Selecting Components\n",
    "To make the decision about which components to keep, we typically keep those that explain the most variance.\n",
    "\n",
    "NOTE: the eigenvalues/S should be sorted descending before performing these steps (may already be done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.1 Variance via Eigendecomposition\n",
    "Explained variance is simply each eigenvalue as a proportion of the sum of the eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prop_eig = [e/sum(eigvals) for e in eigvals]\n",
    "\n",
    "print(sum(prop_eig))\n",
    "print(prop_eig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.2 Variance via SVD\n",
    "Explained variance is each value in S **squared** divided by the sum of all values **squared**:\n",
    "\n",
    "$$ \\frac{s^2}{\\sum_i s_i^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement for SVD\n",
    "prop_svd = None\n",
    "\n",
    "print(sum(prop_svd))\n",
    "print(prop_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.3 Plot Explained Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot([x+1 for x in range(len(prop_eig))], np.cumsum(prop_eig), '-o')\n",
    "\n",
    "plt.yticks(np.linspace(0, 1, 11, endpoint=True))\n",
    "plt.xticks(np.linspace(1, len(prop_eig), len(prop_eig), endpoint=True))\n",
    "plt.title('Cumulative Explained Variance vs Components')\n",
    "\n",
    "np.cumsum(prop_eig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: set the following variable to the number of components needed to explain 90% of the variance\n",
    "numpcs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.4 PCA via `scikit-learn`\n",
    "Now that we've done this by hand, notice the easy shortcut :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skl_pca = sklearn.decomposition.PCA(n_components=numpcs).fit(X_std)\n",
    "\n",
    "print(\"Components:\", skl_pca.components_)\n",
    "print()\n",
    "print(\"S:\", skl_pca.singular_values_)\n",
    "print()\n",
    "print(\"Explained Variance: \", skl_pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G. Visualizing the Projected Data\n",
    "Now let's project the data into the space defined by our top components (where $W_k$ is the matrix of the $k$ top eigenvectors): $ X_{proj} = X_{std}W_k $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: extract the first k eigenvectors\n",
    "W = None\n",
    "\n",
    "print(\"W_{}={}\".format(numpcs, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "print(X_std.shape)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: matrix multiply! (remember @ for shortcut)\n",
    "X_proj = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X_proj[:,0], X_proj[:,1], '.')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G.1 Supervised Visualization\n",
    "Since we have the actual species, let's overlay this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for species in distinct_species:\n",
    "    plt.plot([X_proj[p, 0] for p in Y[Y.isin([species])].index], [X_proj[p, 1] for p in Y[Y.isin([species])].index], '.', label=species)\n",
    "    \n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H. Interpretting the Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one = pd.Series(eigvecs[:, 0], index=X.columns)\n",
    "one.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "two = pd.Series(eigvecs[:, 1], index=X.columns)\n",
    "two.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: based upon the top two basis vectors, what can we generally say about the relative size of Setosa petals/sepals?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
